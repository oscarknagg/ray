cluster_name: example-cluster

max_workers: 63

upscaling_speed: 1.0

idle_timeout_minutes: 1

provider:
    type: kubernetes
    use_internal_ips: true
    namespace: ray

    autoscaler_service_account:
        apiVersion: v1
        kind: ServiceAccount
        metadata:
            name: autoscaler

    autoscaler_role:
        kind: Role
        apiVersion: rbac.authorization.k8s.io/v1
        metadata:
            name: autoscaler
        rules:
        - apiGroups: [""]
          resources: ["pods", "pods/status", "pods/exec"]
          verbs: ["get", "watch", "list", "create", "delete", "patch"]

    autoscaler_role_binding:
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
            name: autoscaler
        subjects:
        - kind: ServiceAccount
          name: autoscaler
        roleRef:
            kind: Role
            name: autoscaler
            apiGroup: rbac.authorization.k8s.io

    services:
      - apiVersion: v1
        kind: Service
        metadata:
            # NOTE: If you're running multiple Ray clusters with services
            # on one Kubernetes cluster, they must have unique service
            # names.
            name: ray-head
        spec:
            # This selector must match the head node pod's selector below.
            selector:
                component: ray-head
            ports:
                - name: client
                  protocol: TCP
                  port: 10001
                  targetPort: 10001
                - name: dashboard
                  protocol: TCP
                  port: 8265
                  targetPort: 8265
                - name: ray-serve
                  protocol: TCP
                  port: 8000
                  targetPort: 8000
                - name: jupyter
                  protocol: TCP
                  port: 8888
                  targetPort: 8888

head_node_type: head_node
available_node_types:
  gpu_node:
    min_workers: 1
    max_workers: 1
    resources: { "CPU": 8, "GPU": 0 }
    node_config:
      apiVersion: v1
      kind: Pod
      metadata:
        # Automatically generates a name for the pod with this prefix.
        generateName: ray-gpu-worker-
        labels:
          component: ray-gpu
      spec:
        restartPolicy: Never
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory

        containers:
        - name: ray-node
          imagePullPolicy: IfNotPresent
          image: rayproject/ray:3bc5f0-py38
          command: ["/bin/bash", "-c", "--"]
          args: ["trap : TERM INT; sleep infinity & wait;"]
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm

          resources:
            limits:
              cpu: 7
              memory: 31G

          # Add in my hacky environment variables to enable TLS
          env:
            - name: RAY_USE_TLS
              value: "1"
            - name: RAY_TLS_SERVER_CERT
              value: "/home/ray/.ssh/server.crt"
            - name: RAY_TLS_SERVER_KEY
              value: "/home/ray/.ssh/server.key"
            - name: RAY_TLS_CA_CERT
              value: "/home/ray/.ssh/ca.crt"
            - name: GRPC_VERBOSITY
              value: "debug"

  head_node:
    min_workers: 0
    max_workers: 0
    resources: { "CPU": 8, "GPU": 0 }
    node_config:
      apiVersion: v1
      kind: Pod
      metadata:
        generateName: ray-head-
        labels:
            component: ray-head
      spec:
        serviceAccountName: autoscaler

        restartPolicy: Never

        volumes:
        - name: dshm
          emptyDir:
            medium: Memory

        containers:
        - name: ray-node
          imagePullPolicy: IfNotPresent
          image: rayproject/ray:3bc5f0-py38
          # Do not change this command - it keeps the pod alive until it is
          # explicitly killed.
          command: ["/bin/bash", "-c", "--"]
          args: ['trap : TERM INT; sleep infinity & wait;']
          ports:
          - containerPort: 6379  # Redis port
          - containerPort: 10001  # Used by Ray Client
          - containerPort: 8265  # Used by Ray Dashboard
          - containerPort: 8888  # Used by Jupyter

          volumeMounts:
          - mountPath: /dev/shm
            name: dshm

          resources:
            limits:
              cpu: 7
              memory: 31G

          env:
            - name: RAY_USE_TLS
              value: "1"
            - name: RAY_TLS_SERVER_CERT
              value: "/home/ray/.ssh/server.crt"
            - name: RAY_TLS_SERVER_KEY
              value: "/home/ray/.ssh/server.key"
            - name: RAY_TLS_CA_CERT
              value: "/home/ray/.ssh/ca.crt"
            - name: GRPC_VERBOSITY
              value: "debug"

setup_commands:
  - openssl req -newkey rsa:2048 -nodes -keyout /home/ray/.ssh/server.key -subj "/C=CN/ST=GD/L=SZ/O=G-Research/CN=$HOSTNAME" -out /home/ray/.ssh/server.csr
  - export SAN=$(python -c "import socket; print(socket.gethostbyname(socket.gethostname()))") && echo subjectAltName=DNS:$SAN,DNS:127.0.0.1,DNS:0.0.0.0,DNS:localhost > /home/ray/.ssh/extfile
  - openssl x509 -req -extfile /home/ray/.ssh/extfile -days 365 -in /home/ray/.ssh/server.csr -CA /home/ray/.ssh/ca.crt -CAkey /home/ray/.ssh/ca.key -CAcreateserial -out /home/ray/.ssh/server.crt
  - pip install /home/ray/ray-2.0.0.dev0-cp38-cp38-linux_x86_64.whl --force-reinstall

# After startup, run this on local machine to pull the keys
# ray rsync_down eks-cluster.yaml /home/ray/.ssh/server.crt /home/oscar/.ssh/server.crt
# ray rsync_down eks-cluster.yaml /home/ray/.ssh/server.key /home/oscar/.ssh/server.key

head_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --head --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host 0.0.0.0 --object-store-memory 16000000000

worker_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-store-memory 16000000000

file_mounts: {
    # TLS enabled Ray wheel
    "/home/ray/ray-2.0.0.dev0-cp38-cp38-linux_x86_64.whl": "/home/oscar/CLionProjects/ray/python/dist/ray-2.0.0.dev0-cp38-cp38-linux_x86_64.whl",
    # Certificate authority keys generated locally
    "/home/ray/.ssh/ca.crt": "/home/oscar/.ssh/ca.crt",
    "/home/ray/.ssh/ca.key": "/home/oscar/.ssh/ca.key",
#    "/path2/on/remote/machine": "/path2/on/local/machine",
}